{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "model: Neural Language Models (Bengio et al. 2003) URL: https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf\n",
    "\n",
    "Dimension key:\n",
    "# windows\n",
    "B: batch size\n",
    "T: sequence length\n",
    "\n",
    "# input/output\n",
    "V: vocabulary size\n",
    "E: embedding dimension\n",
    "D: model dimension\n",
    "\"\"\"\n",
    "import picograd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# from jaxtyping import ...\n",
    "g = picograd.Generator().manual_seed(1337) # for .randn()\n",
    "\n",
    "B, T = 32, 3\n",
    "V, E, D = 27, 10, 200\n",
    "\n",
    "# step: 0/200000, loss 27.63208770751953\n",
    "# -> expected loss = nll = p(c) = -picograd.tensor(1/V=27).log() = 3.2958\n",
    "# -> self.W = picograd.randn() is sampling from N(0, 1)\n",
    "# -> self.W * [gain/sqrt(D_in)] (picograd.init_kaimingnormal())\n",
    "\n",
    "# residuals + normalization + Adam/RMSprop has made initialization less fragile\n",
    "# -> b/c initialization is fragile/intractable with *deep* neural networks\n",
    "\n",
    "class Linear:\n",
    "    def __init__(self, D_in, D_out, bias=True):\n",
    "        self.W_DiDo = picograd.randn((D_in, D_out), generator=g) * (5/3)/D_in**0.5 # kaiming init (He et al. 2015)\n",
    "        self.b_Do = picograd.zeros(D_out) if bias else None\n",
    "\n",
    "    def __call__(self, X_Di):\n",
    "        self.X_Do = X_Di @ self.W_DiDo\n",
    "        if self.b_Do is not None:\n",
    "            self.X_Do += self.b_Do\n",
    "        self.out = self.X_Do\n",
    "        return self.X_Do\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.W_DiDo] + ([] if self.b_Do is None else [self.b_Do])\n",
    "\n",
    "class BatchNorm1D:\n",
    "  \n",
    "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "    self.eps = eps\n",
    "    self.momentum = momentum\n",
    "    self.training = True\n",
    "    # parameters (trained with backprop)\n",
    "    self.gamma = picograd.ones(dim)\n",
    "    self.beta = picograd.zeros(dim)\n",
    "    # buffers (trained with a running 'momentum update')\n",
    "    self.running_mean = picograd.zeros(dim)\n",
    "    self.running_var = picograd.ones(dim)\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    # calculate the forward pass\n",
    "    if self.training:\n",
    "      xmean = x.mean(0, keepdim=True) # batch mean\n",
    "      xvar = x.var(0, keepdim=True) # batch variance\n",
    "    else:\n",
    "      xmean = self.running_mean\n",
    "      xvar = self.running_var\n",
    "    xhat = (x - xmean) / picograd.sqrt(xvar + self.eps) # normalize to unit variance\n",
    "    self.out = self.gamma * xhat + self.beta\n",
    "    # update the buffers\n",
    "    if self.training:\n",
    "      with picograd.no_grad():\n",
    "        self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n",
    "        self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    return [self.gamma, self.beta]\n",
    "\n",
    "class Tanh:\n",
    "    def __call__(self, X_BD):\n",
    "        self.X_BD = picograd.tanh(X_BD)\n",
    "        # plt.hist(self.X_BD.view(-1).tolist(), 50); # distribution of weights\n",
    "        # plt.imshow(self.X_BD.abs() > 0.99, cmap='gray', interpolation='nearest') # vanishing gradients\n",
    "        self.out = self.X_BD\n",
    "        return self.X_BD\n",
    "    \n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "model = [\n",
    "    Linear(T * E, D, bias=False), BatchNorm1D(D), Tanh(),\n",
    "    Linear(D, D, bias=False), BatchNorm1D(D), Tanh(),\n",
    "    Linear(D, V, bias=False), BatchNorm1D(V)\n",
    "]\n",
    "\n",
    "C = picograd.randn((V,E), generator=g)\n",
    "params = [C] + [p for l in model for p in l.parameters()]\n",
    "for p in params:\n",
    "    p.requires_grad = True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
