{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.9507, -0.6767,  0.6160,  1.4352,  1.2668, -0.3695],\n",
      "         [ 2.1627,  0.5298, -0.1970, -0.9908,  0.5932,  0.5915],\n",
      "         [ 1.1021,  2.3204,  1.5712,  0.7537, -0.1879, -1.1767],\n",
      "         [-1.1242,  0.9053, -1.5349,  0.5488,  0.6083, -0.5120]],\n",
      "\n",
      "        [[-0.4691,  0.5749,  1.8767,  1.0280, -0.0382, -1.2889],\n",
      "         [-1.2532, -0.8635,  0.1307,  0.1539,  1.4230,  0.4671],\n",
      "         [ 0.2053, -0.5525,  0.5871,  1.6242,  0.5533, -0.2852],\n",
      "         [-1.2761, -1.6750, -1.4106, -0.9994,  0.2580,  1.8726]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.randn(2, 4, 6)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "model: Neural Language Models (Bengio et al. 2003) URL: https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf\n",
    "\n",
    "Dimension key:\n",
    "# windows\n",
    "B: batch size\n",
    "T: sequence length\n",
    "\n",
    "# input/output\n",
    "V: vocabulary size\n",
    "E: embedding dimension\n",
    "D: model dimension\n",
    "\"\"\"\n",
    "import picograd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# from jaxtyping import ...\n",
    "g = picograd.Generator().manual_seed(1337) # for .randn()\n",
    "\n",
    "B, T = 32, 3\n",
    "V, E, D = 27, 10, 200\n",
    "\n",
    "# step: 0/200000, loss 27.63208770751953\n",
    "# -> expected loss = nll = p(c) = -picograd.tensor(1/V=27).log() = 3.2958\n",
    "# -> self.W = picograd.randn() is sampling from N(0, 1)\n",
    "# -> self.W * [gain/sqrt(D_in)] (picograd.init_kaimingnormal())\n",
    "\n",
    "# residuals + normalization + Adam/RMSprop has made initialization less fragile\n",
    "# -> b/c initialization is fragile/intractable with *deep* neural networks\n",
    "\n",
    "class Linear:\n",
    "    def __init__(self, D_in, D_out, bias=True):\n",
    "        self.W_DiDo = picograd.randn((D_in, D_out), generator=g) * (5/3)/D_in**0.5 # kaiming init (He et al. 2015)\n",
    "        self.b_Do = picograd.zeros(D_out) if bias else None\n",
    "\n",
    "    def __call__(self, X_Di):\n",
    "        self.X_Do = X_Di @ self.W_DiDo\n",
    "        if self.b_Do is not None:\n",
    "            self.X_Do += self.b_Do\n",
    "        self.out = self.X_Do\n",
    "        return self.X_Do\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.W_DiDo] + ([] if self.b_Do is None else [self.b_Do])\n",
    "\n",
    "class BatchNorm1D:\n",
    "  \n",
    "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "    self.eps = eps\n",
    "    self.momentum = momentum\n",
    "    self.training = True\n",
    "    # parameters (trained with backprop)\n",
    "    self.gamma = picograd.ones(dim)\n",
    "    self.beta = picograd.zeros(dim)\n",
    "    # buffers (trained with a running 'momentum update')\n",
    "    self.running_mean = picograd.zeros(dim)\n",
    "    self.running_var = picograd.ones(dim)\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    # calculate the forward pass\n",
    "    if self.training:\n",
    "      xmean = x.mean(0, keepdim=True) # batch mean\n",
    "      xvar = x.var(0, keepdim=True) # batch variance\n",
    "    else:\n",
    "      xmean = self.running_mean\n",
    "      xvar = self.running_var\n",
    "    xhat = (x - xmean) / picograd.sqrt(xvar + self.eps) # normalize to unit variance\n",
    "    self.out = self.gamma * xhat + self.beta\n",
    "    # update the buffers\n",
    "    if self.training:\n",
    "      with picograd.no_grad():\n",
    "        self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n",
    "        self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    return [self.gamma, self.beta]\n",
    "\n",
    "class Tanh:\n",
    "    def __call__(self, X_BD):\n",
    "        self.X_BD = picograd.tanh(X_BD)\n",
    "        # plt.hist(self.X_BD.view(-1).tolist(), 50); # distribution of weights\n",
    "        # plt.imshow(self.X_BD.abs() > 0.99, cmap='gray', interpolation='nearest') # vanishing gradients\n",
    "        self.out = self.X_BD\n",
    "        return self.X_BD\n",
    "    \n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "model = [\n",
    "    Linear(T * E, D, bias=False), BatchNorm1D(D), Tanh(),\n",
    "    Linear(D, D, bias=False), BatchNorm1D(D), Tanh(),\n",
    "    Linear(D, V, bias=False), BatchNorm1D(V)\n",
    "]\n",
    "\n",
    "C = picograd.randn((V,E), generator=g)\n",
    "params = [C] + [p for l in model for p in l.parameters()]\n",
    "for p in params:\n",
    "    p.requires_grad = True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
