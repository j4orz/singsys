{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32) torch.int32\n",
      "tensor([2.7183, 2.7183, 2.7183, 2.7183, 2.7183, 2.7183, 2.7183, 2.7183, 2.7183]) torch.float32\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.ones(9, dtype=torch.int)\n",
    "y = torch.exp(x)\n",
    "print(x, x.dtype)\n",
    "print(y, y.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "model: Neural Language Models (Bengio et al. 2003) URL: https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf\n",
    "\n",
    "Dimension key:\n",
    "# windows\n",
    "B: batch size\n",
    "T: sequence length\n",
    "\n",
    "# input/output\n",
    "V: vocabulary size\n",
    "E: embedding dimension\n",
    "D: model dimension\n",
    "\"\"\"\n",
    "import picograd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# from jaxtyping import ...\n",
    "# g = picograd.Generator().manual_seed(1337) # for .randn()\n",
    "\n",
    "B, T = 32, 3\n",
    "V, E, D = 27, 10, 200\n",
    "\n",
    "# step: 0/200000, loss 27.63208770751953\n",
    "# -> expected loss = nll = p(c) = -picograd.tensor(1/V=27).log() = 3.2958\n",
    "# -> self.W = picograd.randn() is sampling from N(0, 1)\n",
    "# -> self.W * [gain/sqrt(D_in)] (picograd.init_kaimingnormal())\n",
    "\n",
    "# residuals + normalization + Adam/RMSprop has made initialization less fragile\n",
    "# -> b/c initialization is fragile/intractable with *deep* neural networks\n",
    "\n",
    "class Linear:\n",
    "    def __init__(self, D_in, D_out, bias=True):\n",
    "        self.W_DiDo = picograd.randn((D_in, D_out), generator=g) * (5/3)/D_in**0.5 # kaiming init (He et al. 2015)\n",
    "        self.b_Do = picograd.zeros(D_out) if bias else None\n",
    "\n",
    "    def __call__(self, X_Di):\n",
    "        self.X_Do = X_Di @ self.W_DiDo\n",
    "        if self.b_Do is not None:\n",
    "            self.X_Do += self.b_Do\n",
    "        self.out = self.X_Do\n",
    "        return self.X_Do\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.W_DiDo] + ([] if self.b_Do is None else [self.b_Do])\n",
    "\n",
    "class Tanh:\n",
    "    def __call__(self, X_BD):\n",
    "        self.X_BD = picograd.tanh(X_BD)\n",
    "        # plt.hist(self.X_BD.view(-1).tolist(), 50); # distribution of weights\n",
    "        # plt.imshow(self.X_BD.abs() > 0.99, cmap='gray', interpolation='nearest') # vanishing gradients\n",
    "        self.out = self.X_BD\n",
    "        return self.X_BD\n",
    "    \n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "# model = [\n",
    "#     Linear(T * E, D, bias=False), BatchNorm1D(D), Tanh(),\n",
    "#     Linear(D, D, bias=False), BatchNorm1D(D), Tanh(),\n",
    "#     Linear(D, V, bias=False), BatchNorm1D(V)\n",
    "# ]\n",
    "\n",
    "model = [\n",
    "    Linear(T * E, D, bias=False), Tanh(),\n",
    "    Linear(D, D, bias=False), Tanh(),\n",
    "    Linear(D, V, bias=False)\n",
    "]\n",
    "\n",
    "C = picograd.randn((V,E), generator=g)\n",
    "params = [C] + [p for l in model for p in l.parameters()]\n",
    "for p in params:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. dataloader\n",
    "import picograd.nn.functional as F\n",
    "import random\n",
    "\n",
    "words = open('./data/names.txt', 'r').read().splitlines()\n",
    "v = sorted(list(set(''.join(words))))\n",
    "encode = { c:i+1 for i,c in enumerate(v) }\n",
    "encode['.'] = 0\n",
    "decode = { i:c for c,i in encode.items() }\n",
    "\n",
    "def gen_dataset(words):\n",
    "    X, Y = [], []\n",
    "    for w in words:\n",
    "        context = [0] * T;\n",
    "        for c in w + '.':\n",
    "            X.append(context)\n",
    "            Y.append(encode[c])\n",
    "            # print(''.join(decode[i] for i in context), '-->', decode[encode[c]])\n",
    "            context = context[1:] + [encode[c]]\n",
    "    X, Y = picograd.tensor(X), picograd.tensor(Y) # X:(N,C) Y:(N)\n",
    "    return X, Y\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1, n2 = int(0.8*len(words)), int(0.9*len(words))\n",
    "Xtr, Ytr = gen_dataset(words[:n1])\n",
    "Xdev, Ydev = gen_dataset(words[n1:n2])\n",
    "Xte, Yte = gen_dataset(words[n2:])\n",
    "\n",
    "# 2. training loop\n",
    "N = Xtr.shape[0]\n",
    "losses, steps = [], []\n",
    "for step in range(200000):\n",
    "    # 1. forward\n",
    "    indices_B = picograd.randint(0, N, (B,))\n",
    "    X_B, Y_B = Xtr[indices_B], Ytr[indices_B]\n",
    "\n",
    "    X_BD = C[X_B].view(-1, T * E)\n",
    "    for layer in model:\n",
    "        X_BD = layer(X_BD)\n",
    "    loss = F.cross_entropy(X_BD, Y_B)\n",
    "\n",
    "    # 2. backward\n",
    "    for layer in model:\n",
    "        layer.out.retain_grad()\n",
    "    for p in params:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # 3. update\n",
    "    for p in params:\n",
    "        p.data += -0.01 * p.grad\n",
    "\n",
    "    steps.append(step)\n",
    "    losses.append(loss.log10().item())\n",
    "    if step % 10000 == 0:\n",
    "        print(f\"step: {step}/{200000}, loss {loss.item()}\")\n",
    "\n",
    "plt.plot(steps, losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference loop\n",
    "for layer in model:\n",
    "  if isinstance(layer, BatchNorm1D):\n",
    "      layer.training = False\n",
    "\n",
    "token_terminal = 0\n",
    "for _ in range(20):\n",
    "  output, context = [], [0] * T\n",
    "  while True:\n",
    "      emb = C[picograd.tensor([context])]\n",
    "      X_BD = emb.view(emb.shape[0], -1)\n",
    "      for layer in model:\n",
    "        X_BD = layer(X_BD)\n",
    "      logits = X_BD\n",
    "      probs = F.softmax(logits, dim=1)\n",
    "\n",
    "      token = picograd.multinomial(probs, num_samples=1, replacement=True, generator=g).item()\n",
    "      context = context[1:] + [token]\n",
    "      output.append(decode[token])\n",
    "      if token == token_terminal:\n",
    "          break\n",
    "  print(''.join(output))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
